# Stream Processing

Stream processing is anthoer technique used to process or analyze high volume of data as it arrives. The processing will be real-time our near real-time.

Systems like Kafka is an example of systems that support stream processing. It is designed to handle large volumes of data with minimal delay, making them ideal for scenarios requiring real-time insights, event triggering, fraud detection, and monitoring.

Stream processing is composed of three main components:

- Event Source: These are the systems or devices where the data originates.
- Stream Processors: These components read, analyze, and potentially transform the incoming data.
- Event Sinks: These are the destinations where the processed data is stored, such as another topic, database, data lake, etc.

By constantly processing data in the stream, companies can extract valuable insights and make immediate actions based on the most up-to-date information.

![](/images/17.png)
